{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098b9dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install lightgbm optuna scikit-learn pandas matplotlib seaborn\n",
    "%pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e94518d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BLOCK 1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import lightgbm as lgb\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b20c4f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 2 Load dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\aruna\\OneDrive\\Desktop\\Major-Pro\\DATASET\\Processed\\cleaned_final_data.csv\")\n",
    "\n",
    "# Remove missing target rows\n",
    "df = df.dropna(subset=[\"current_value\"])\n",
    "\n",
    "# Log-transform target to stabilize outliers\n",
    "df[\"current_value\"] = np.log1p(df[\"current_value\"])\n",
    "\n",
    "# Clip extreme outliers at 99th percentile\n",
    "clip_val = df[\"current_value\"].quantile(0.99)\n",
    "df[\"current_value\"] = np.clip(df[\"current_value\"], None, clip_val)\n",
    "\n",
    "# Example engineered feature: Age Bucket\n",
    "df[\"age_bucket\"] = pd.cut(df[\"age\"], bins=[0,20,25,30,35,100],\n",
    "                        labels=[\"<20\",\"20-25\",\"25-30\",\"30-35\",\"35+\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11ad1e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# block 3 Label encode categorical variables\n",
    "cat_cols = [\"team\", \"position\", \"age_bucket\"]\n",
    "le = LabelEncoder()\n",
    "for col in cat_cols:\n",
    "    df[col] = le.fit_transform(df[col].astype(str))\n",
    "\n",
    "# Features & target\n",
    "X = df.drop(columns=[\"current_value\"])\n",
    "y = df[\"current_value\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0ad60f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# block 4\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "908d5d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002001 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2892\n",
      "[LightGBM] [Info] Number of data points in the train set: 8603, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 13.522265\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000950 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2899\n",
      "[LightGBM] [Info] Number of data points in the train set: 8603, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 13.494408\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000950 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2899\n",
      "[LightGBM] [Info] Number of data points in the train set: 8603, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 13.494408\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000874 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2897\n",
      "[LightGBM] [Info] Number of data points in the train set: 8603, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 13.498303\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000874 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2897\n",
      "[LightGBM] [Info] Number of data points in the train set: 8603, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 13.498303\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000936 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2896\n",
      "[LightGBM] [Info] Number of data points in the train set: 8603, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 13.507482\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000936 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2896\n",
      "[LightGBM] [Info] Number of data points in the train set: 8603, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 13.507482\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000867 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2901\n",
      "[LightGBM] [Info] Number of data points in the train set: 8604, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 13.502009\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000867 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2901\n",
      "[LightGBM] [Info] Number of data points in the train set: 8604, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 13.502009\n",
      "Baseline CV R²: 0.49043703426927615\n",
      "Baseline CV R²: 0.49043703426927615\n"
     ]
    }
   ],
   "source": [
    "# BLOCK 5\n",
    "model = lgb.LGBMRegressor(random_state=42)\n",
    "\n",
    "scores = cross_val_score(model, X, y, cv=kf, scoring=\"r2\")\n",
    "print(\"Baseline CV R²:\", scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1957b4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-13 20:22:30,701] A new study created in memory with name: no-name-ebc60c5d-7e27-4130-85f5-9fded4443c4e\n",
      "[I 2025-09-13 20:24:43,600] Trial 0 finished with value: 0.47090773006629344 and parameters: {'learning_rate': 0.01141387591739505, 'num_leaves': 371, 'max_depth': 14, 'min_child_samples': 67, 'subsample': 0.5582720729683084, 'colsample_bytree': 0.7166958871297815, 'lambda_l1': 3.5249018983427676, 'lambda_l2': 0.08600419816041646}. Best is trial 0 with value: 0.47090773006629344.\n",
      "[I 2025-09-13 20:24:43,600] Trial 0 finished with value: 0.47090773006629344 and parameters: {'learning_rate': 0.01141387591739505, 'num_leaves': 371, 'max_depth': 14, 'min_child_samples': 67, 'subsample': 0.5582720729683084, 'colsample_bytree': 0.7166958871297815, 'lambda_l1': 3.5249018983427676, 'lambda_l2': 0.08600419816041646}. Best is trial 0 with value: 0.47090773006629344.\n",
      "[I 2025-09-13 20:25:45,484] Trial 1 finished with value: 0.47863436514891544 and parameters: {'learning_rate': 0.014665536083883594, 'num_leaves': 81, 'max_depth': 9, 'min_child_samples': 57, 'subsample': 0.9848623320279271, 'colsample_bytree': 0.6601431574778038, 'lambda_l1': 4.228465749778768e-05, 'lambda_l2': 1.0114228717175902e-05}. Best is trial 1 with value: 0.47863436514891544.\n",
      "[I 2025-09-13 20:25:45,484] Trial 1 finished with value: 0.47863436514891544 and parameters: {'learning_rate': 0.014665536083883594, 'num_leaves': 81, 'max_depth': 9, 'min_child_samples': 57, 'subsample': 0.9848623320279271, 'colsample_bytree': 0.6601431574778038, 'lambda_l1': 4.228465749778768e-05, 'lambda_l2': 1.0114228717175902e-05}. Best is trial 1 with value: 0.47863436514891544.\n",
      "[I 2025-09-13 20:27:09,997] Trial 2 finished with value: 0.45508229530149985 and parameters: {'learning_rate': 0.020454521561339256, 'num_leaves': 75, 'max_depth': 14, 'min_child_samples': 90, 'subsample': 0.6833847476946269, 'colsample_bytree': 0.7138778028211792, 'lambda_l1': 0.009780685615130897, 'lambda_l2': 4.05637215584533}. Best is trial 1 with value: 0.47863436514891544.\n",
      "[I 2025-09-13 20:27:09,997] Trial 2 finished with value: 0.45508229530149985 and parameters: {'learning_rate': 0.020454521561339256, 'num_leaves': 75, 'max_depth': 14, 'min_child_samples': 90, 'subsample': 0.6833847476946269, 'colsample_bytree': 0.7138778028211792, 'lambda_l1': 0.009780685615130897, 'lambda_l2': 4.05637215584533}. Best is trial 1 with value: 0.47863436514891544.\n",
      "[I 2025-09-13 20:28:23,207] Trial 3 finished with value: 0.42591575802437864 and parameters: {'learning_rate': 0.04973438720879288, 'num_leaves': 206, 'max_depth': 10, 'min_child_samples': 82, 'subsample': 0.5812993258776209, 'colsample_bytree': 0.8871450771454339, 'lambda_l1': 4.61873734635255e-08, 'lambda_l2': 0.022868758641699127}. Best is trial 1 with value: 0.47863436514891544.\n",
      "[I 2025-09-13 20:28:23,207] Trial 3 finished with value: 0.42591575802437864 and parameters: {'learning_rate': 0.04973438720879288, 'num_leaves': 206, 'max_depth': 10, 'min_child_samples': 82, 'subsample': 0.5812993258776209, 'colsample_bytree': 0.8871450771454339, 'lambda_l1': 4.61873734635255e-08, 'lambda_l2': 0.022868758641699127}. Best is trial 1 with value: 0.47863436514891544.\n",
      "[I 2025-09-13 20:29:56,192] Trial 4 finished with value: 0.4869042724175894 and parameters: {'learning_rate': 0.010848570029555235, 'num_leaves': 58, 'max_depth': 15, 'min_child_samples': 13, 'subsample': 0.8424593748097213, 'colsample_bytree': 0.5718437879968258, 'lambda_l1': 0.023922301931493018, 'lambda_l2': 4.110346313409032e-07}. Best is trial 4 with value: 0.4869042724175894.\n",
      "[I 2025-09-13 20:29:56,192] Trial 4 finished with value: 0.4869042724175894 and parameters: {'learning_rate': 0.010848570029555235, 'num_leaves': 58, 'max_depth': 15, 'min_child_samples': 13, 'subsample': 0.8424593748097213, 'colsample_bytree': 0.5718437879968258, 'lambda_l1': 0.023922301931493018, 'lambda_l2': 4.110346313409032e-07}. Best is trial 4 with value: 0.4869042724175894.\n",
      "[I 2025-09-13 20:32:31,120] Trial 5 finished with value: 0.4719803799946072 and parameters: {'learning_rate': 0.015306748926882252, 'num_leaves': 341, 'max_depth': 13, 'min_child_samples': 10, 'subsample': 0.524126892679147, 'colsample_bytree': 0.6863333572310171, 'lambda_l1': 2.890996318291544, 'lambda_l2': 0.00029799666012510966}. Best is trial 4 with value: 0.4869042724175894.\n",
      "[I 2025-09-13 20:32:31,120] Trial 5 finished with value: 0.4719803799946072 and parameters: {'learning_rate': 0.015306748926882252, 'num_leaves': 341, 'max_depth': 13, 'min_child_samples': 10, 'subsample': 0.524126892679147, 'colsample_bytree': 0.6863333572310171, 'lambda_l1': 2.890996318291544, 'lambda_l2': 0.00029799666012510966}. Best is trial 4 with value: 0.4869042724175894.\n",
      "[I 2025-09-13 20:33:38,500] Trial 6 finished with value: 0.4552938003793031 and parameters: {'learning_rate': 0.037230405016218784, 'num_leaves': 34, 'max_depth': 11, 'min_child_samples': 20, 'subsample': 0.6508633668336751, 'colsample_bytree': 0.5790785488257295, 'lambda_l1': 7.25534530124824e-08, 'lambda_l2': 7.385965251801545e-07}. Best is trial 4 with value: 0.4869042724175894.\n",
      "[I 2025-09-13 20:33:38,500] Trial 6 finished with value: 0.4552938003793031 and parameters: {'learning_rate': 0.037230405016218784, 'num_leaves': 34, 'max_depth': 11, 'min_child_samples': 20, 'subsample': 0.6508633668336751, 'colsample_bytree': 0.5790785488257295, 'lambda_l1': 7.25534530124824e-08, 'lambda_l2': 7.385965251801545e-07}. Best is trial 4 with value: 0.4869042724175894.\n",
      "[I 2025-09-13 20:35:04,981] Trial 7 finished with value: 0.40322488623861563 and parameters: {'learning_rate': 0.085219204375549, 'num_leaves': 62, 'max_depth': 12, 'min_child_samples': 41, 'subsample': 0.917480226252956, 'colsample_bytree': 0.6628332265711124, 'lambda_l1': 0.00039468392783898295, 'lambda_l2': 0.0008356006521068251}. Best is trial 4 with value: 0.4869042724175894.\n",
      "[I 2025-09-13 20:35:04,981] Trial 7 finished with value: 0.40322488623861563 and parameters: {'learning_rate': 0.085219204375549, 'num_leaves': 62, 'max_depth': 12, 'min_child_samples': 41, 'subsample': 0.917480226252956, 'colsample_bytree': 0.6628332265711124, 'lambda_l1': 0.00039468392783898295, 'lambda_l2': 0.0008356006521068251}. Best is trial 4 with value: 0.4869042724175894.\n",
      "[I 2025-09-13 20:35:37,559] Trial 8 finished with value: 0.4702247368004369 and parameters: {'learning_rate': 0.059399787794524254, 'num_leaves': 248, 'max_depth': 4, 'min_child_samples': 70, 'subsample': 0.6145788853422061, 'colsample_bytree': 0.6496955957289574, 'lambda_l1': 0.002231365786559433, 'lambda_l2': 0.0008465284249293385}. Best is trial 4 with value: 0.4869042724175894.\n",
      "[I 2025-09-13 20:35:37,559] Trial 8 finished with value: 0.4702247368004369 and parameters: {'learning_rate': 0.059399787794524254, 'num_leaves': 248, 'max_depth': 4, 'min_child_samples': 70, 'subsample': 0.6145788853422061, 'colsample_bytree': 0.6496955957289574, 'lambda_l1': 0.002231365786559433, 'lambda_l2': 0.0008465284249293385}. Best is trial 4 with value: 0.4869042724175894.\n",
      "[I 2025-09-13 20:36:11,970] Trial 9 finished with value: 0.48991169312820926 and parameters: {'learning_rate': 0.020985435025234467, 'num_leaves': 280, 'max_depth': 5, 'min_child_samples': 35, 'subsample': 0.6468500139267441, 'colsample_bytree': 0.652167494030897, 'lambda_l1': 7.724505697345126e-07, 'lambda_l2': 1.535513333019838e-06}. Best is trial 9 with value: 0.48991169312820926.\n",
      "[I 2025-09-13 20:36:11,970] Trial 9 finished with value: 0.48991169312820926 and parameters: {'learning_rate': 0.020985435025234467, 'num_leaves': 280, 'max_depth': 5, 'min_child_samples': 35, 'subsample': 0.6468500139267441, 'colsample_bytree': 0.652167494030897, 'lambda_l1': 7.724505697345126e-07, 'lambda_l2': 1.535513333019838e-06}. Best is trial 9 with value: 0.48991169312820926.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'learning_rate': 0.020985435025234467, 'num_leaves': 280, 'max_depth': 5, 'min_child_samples': 35, 'subsample': 0.6468500139267441, 'colsample_bytree': 0.652167494030897, 'lambda_l1': 7.724505697345126e-07, 'lambda_l2': 1.535513333019838e-06}\n",
      "Best CV R²: 0.48991169312820926\n"
     ]
    }
   ],
   "source": [
    "# BLOCK 6 — Hyperparameter tuning with Optuna (using callbacks)\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"objective\": \"regression\",\n",
    "        \"metric\": \"rmse\",\n",
    "        \"verbosity\": -1,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"n_estimators\": 5000,\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 31, 512),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 20),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
    "    }\n",
    "    \n",
    "    cv_scores = cross_val_score(\n",
    "        lgb.LGBMRegressor(**params),\n",
    "        X, y, cv=kf, scoring=\"r2\"\n",
    "    )\n",
    "    return cv_scores.mean()\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=10)\n",
    "print(\"Best Params:\", study.best_params)\n",
    "print(\"Best CV R²:\", study.best_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b2fe7df",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "lightgbm.sklearn.LGBMRegressor() got multiple values for keyword argument 'n_estimators'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# BLOCK 7 Train with validation set and early stopping\u001b[39;00m\n\u001b[0;32m      2\u001b[0m X_train, X_valid, y_train, y_valid \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[0;32m      3\u001b[0m     X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[0;32m      4\u001b[0m )\n\u001b[1;32m----> 6\u001b[0m final_model \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mLGBMRegressor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbest_params, n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      8\u001b[0m final_model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m      9\u001b[0m     X_train, y_train,\n\u001b[0;32m     10\u001b[0m     eval_set\u001b[38;5;241m=\u001b[39m[(X_valid, y_valid)],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest iteration (epochs):\u001b[39m\u001b[38;5;124m\"\u001b[39m, final_model\u001b[38;5;241m.\u001b[39mbest_iteration_)\n",
      "\u001b[1;31mTypeError\u001b[0m: lightgbm.sklearn.LGBMRegressor() got multiple values for keyword argument 'n_estimators'"
     ]
    }
   ],
   "source": [
    "# BLOCK 7 Train with validation set and early stopping\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "final_model = lgb.LGBMRegressor(**best_params, n_estimators=5000, random_state=42)\n",
    "\n",
    "final_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_valid, y_valid)],\n",
    "    eval_metric=\"rmse\",\n",
    "    early_stopping_rounds=100,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"Best iteration (epochs):\", final_model.best_iteration_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815b9675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test set\n",
    "y_pred = final_model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"MAE:\", mae)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R²:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566ce552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undo the log1p transform\n",
    "y_test_exp = np.expm1(y_test)\n",
    "y_pred_exp = np.expm1(y_pred)\n",
    "\n",
    "mae = mean_absolute_error(y_test_exp, y_pred_exp)\n",
    "rmse = mean_squared_error(y_test_exp, y_pred_exp, squared=False)\n",
    "r2 = r2_score(y_test_exp, y_pred_exp)\n",
    "\n",
    "print(\"MAE (original scale):\", mae)\n",
    "print(\"RMSE (original scale):\", rmse)\n",
    "print(\"R²:\", r2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
